# -*- coding: utf-8 -*-
"""subpopulation_adult_Suya.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fF85onHUcLrHM8q3Ymi1RWWbiKHYzobQ
"""

# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data
# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test
import numpy as np
import pandas as pd
from sklearn import neural_network, linear_model, cluster
from sklearn.externals import joblib
import sys
np.random.seed(0)

FeatMatch_size_min = 100
FeatMatch_size_max = 130
FeatMatch_var_min = 0
FeatMatch_var_max = 10

# define distance computaton related functions
def compute_dist(src, dest,disc_idx= None):
  # compute the distance of two points, if there are discrete indices: compute their hamming distance
  if disc_idx is not None:
    dist = np.linalg.norm(src[np.logical_not(disc_idx)]-dest[np.logical_not(disc_idx)]) + np.linalg.norm(src[disc_idx] - dest[disc_idx],ord = 1)
    return dist
  else:
    return np.linalg.norm(src-dest)

def compactness(subpop):
  # find the compactness of the given subpopulation
  # calculate the variance of the data points
  mean = np.mean(subpop,axis = 0)
  dist = 0
  for i in range(len(subpop)):
    dist += compute_dist(subpop[i], mean)**2 
  return dist/len(subpop)

def separability(subpop,restpops, metric = 'min'):
  # find the separability of given points from the 
  dist = []
  for restpop in restpops:
    if metric == 'min':# max distance of two points, each in the cluster
      min_dist = 1e10
      for i in range(len(subpop)):
        for j in range(len(restpop)):
          if compute_dist(subpop[i],restpop[j]) < min_dist:
            min_dist = compute_dist(subpop[i],restpop[j])
      dist.append(min_dist)
    elif metric == 'max':# max distance of two points, each in the cluster, this is the equivalent to cohension metric
      max_dist = 0
      for i in range(len(subpop)):
        for j in range(len(restpop)):
          if compute_dist(subpop[i],restpop[j]) > max_dist:
            max_dist = compute_dist(subpop[i],restpop[j])
      dist.append(max_dist)

    elif metric == 'avg': # average distance of points in two clusters, equivalent to avg cohension metric 
      avg_dist = 0
      for i in range(len(subpop)):
        for j in range(len(restpop)):
          avg_dist += compute_dist(subpop[i],restpop[j])
      avg_dist = avg_dist/(len(subpop)*len(restpop))
      dist.append(avg_dist)
  return dist

##############################################################################
########################## Start the Real Analysis Part ######################
##############################################################################
FeatureMatch = True
Cluster = True
cluster_wise = False
train_model_cluster = False
train_model_feature = False
train_model = False

compute_metric_cluster = False
compute_metric_feature = False

a = pd.read_csv('adult.data', header=None,
                names=['age', 'workclass', 'fnlwgt', 'education',
                       'education-num', 'marital-status', 'occupation', 'relationship',
                       'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',
                      'native-country', 'income'])
print(a.shape)
b = pd.read_csv('adult.test', header=None,
                names=['age', 'workclass', 'fnlwgt', 'education',
                       'education-num', 'marital-status', 'occupation', 'relationship',
                       'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',
                      'native-country', 'income'])
print(b.shape)
full = pd.concat([a, b], axis=0)
print(full.shape)

full = full.drop('education', axis=1)
full = full.drop('native-country', axis=1)
full = full.drop('fnlwgt', axis=1)
for col in ['workclass', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'income']:
  if '-' in col:
    prefix_col = col.split('-')[0]
  else:
    prefix_col = col
  # kind of converting into one-hot encoding, basically coverting categorical values into one-hott encoding
  full = pd.concat([full, pd.get_dummies(full[col], prefix=prefix_col, drop_first=True)], axis=1)
  full = full.drop(col, axis=1)

# normalize the numeric features
cols_to_norm = ['capital-gain','capital-loss','hours-per-week']
full[cols_to_norm] = full[cols_to_norm].apply(lambda x: (x - x.min()) / x.max())

# print(full.shape)
# print(list(full.columns))
# print(full.tail())
# print("statistics of data")
# print(full.describe())

# ‘age' is somewhat hard to normalize, so, will normalize it in the numpy array’
full_np = full.to_numpy()
y = (full_np[:, -1] + full_np[:, -2]).astype(np.float32)
y = np.delete(y, 32561, axis=0) # this row is '1x3 Cross validator' and should be removed
x = np.delete(full_np, [full_np.shape[1]-1, full_np.shape[1]-2, full_np.shape[1]-3], axis=1)
x = np.delete(x, 32561, axis=0).astype(np.float32)


x[:,0] = (x[:,0]-np.amin(x[:,0]))/(np.amax(x[:,0])-np.amin(x[:,0]))

trn_x, trn_y = x[:32561], y[:32561]
tst_x, tst_y = x[32561:], y[32561:]

trn_zero_inds = np.where(trn_y==0)[0]
trn_one_inds = np.where(trn_y==1)[0]
tst_zero_inds = np.where(tst_y==0)[0]
tst_one_inds = np.where(tst_y==1)[0]

print("before any edits:",trn_x.shape)

disc_idx = np.ones(trn_x.shape[1])
disc_idx[0:4] = 0 # first four features are numerical values

# subsampling to make the dataset balanced
trn_zeros = np.random.choice(trn_zero_inds.shape[0], trn_one_inds.shape[0], replace=False)
tst_zeros = np.random.choice(tst_zero_inds.shape[0], tst_one_inds.shape[0], replace=False)

trn_x = np.concatenate((trn_x[trn_zeros], trn_x[trn_one_inds]), axis=0)
tst_x = np.concatenate((tst_x[tst_zeros], tst_x[tst_one_inds]), axis=0)
trn_y = np.concatenate((trn_y[trn_zeros], trn_y[trn_one_inds]), axis=0)
tst_y = np.concatenate((tst_y[tst_zeros], tst_y[tst_one_inds]), axis=0)

trn_shuffle = np.random.choice(trn_x.shape[0], trn_x.shape[0], replace=False)
trn_x, trn_y = trn_x[trn_shuffle], trn_y[trn_shuffle]

pois_rates = [1/3, 2/3, 1, 4/3, 5/3, 2]
#pois_rates = [0.5, 1, 1.5, 2, 2.5, 3]

print(trn_x.shape, trn_y.shape, tst_x.shape, tst_y.shape)

## train model on clean data
if train_model:
  lm = linear_model.LogisticRegression()
  lm.fit(trn_x, trn_y)
  # save the clean logistic regression model
  filename = 'models/clean_lr.sav'
  joblib.dump(lm, filename)
  # predict the values
  lm_preds = np.multiply(lm.predict_proba(trn_x), np.eye(2)[trn_y.astype(np.int)]).sum(axis=1)
  print(lm.score(tst_x, tst_y))

  nn = neural_network.MLPClassifier(hidden_layer_sizes=(10,))
  nn.fit(trn_x, trn_y)
  # store the clean neural network model
  filename = 'models/clean_nn.sav'
  joblib.dump(nn, filename)
  # predict the values
  nn_preds = np.multiply(nn.predict_proba(trn_x), np.eye(2)[trn_y.astype(np.int)]).sum(axis=1)
  print(nn.score(tst_x, tst_y))

else:
  filename = 'models/clean_lr.sav'
  lm = joblib.load(open(filename, 'rb'))
  print(lm.score(tst_x, tst_y))
  # predict the values
  lm_preds = np.multiply(lm.predict_proba(trn_x), np.eye(2)[trn_y.astype(np.int)]).sum(axis=1)
  print(lm.score(tst_x, tst_y))

  filename = 'models/clean_nn.sav'
  nn = joblib.load(open(filename, 'rb'))
  print(nn.score(tst_x, tst_y))
  # predict the values
  nn_preds = np.multiply(nn.predict_proba(trn_x), np.eye(2)[trn_y.astype(np.int)]).sum(axis=1)
  print(nn.score(tst_x, tst_y))


#### this is the part of FeatureMatch subpopulation ########
# protected = np.concatenate((trn_x[:, 12:27], trn_x[:, 52:57]), axis=1)

tst_prot = np.concatenate((tst_x[:, 12:27], tst_x[:, 52:57]), axis=1)
trn_prot = np.concatenate((trn_x[:, 12:27], trn_x[:, 52:57]), axis=1)
all_cols = list(full.columns)
prot_cols = all_cols[12:27] + all_cols[52:57]
# because of small # of features, lots of repetitive instances
subclasses, counts = np.unique(trn_prot, axis=0, return_counts=True)

from scipy import stats
print(counts)
print(stats.mode(counts))
sys.exit(0)
hand_designed = []
hand_designed_metric = []


if FeatureMatch:
  feat_names = []
  ##################### Starts the process of clustering based feature match ################
  for i, (subcl, count) in enumerate(zip(subclasses, counts)):
    if count > subpop_size_min and count < subpop_size_max:
      # one-hot valued features, so l2 dist == 0 means the specific subpopulation
      tst_sbcl = np.where(np.linalg.norm(tst_prot - subcl, axis=1)==0)
      trn_sbcl = np.where(np.linalg.norm(trn_prot-subcl, axis=1)==0)

      # incides of non-subclass
      tst_non_sbcl = np.where(np.linalg.norm(tst_prot - subcl, axis=1)!=0)
      trn_non_sbcl = np.where(np.linalg.norm(trn_prot-subcl, axis=1)!=0)

      # istances in the subpopulation and rest of subpop
      p_t_x, p_t_y = tst_x[tst_sbcl], tst_y[tst_sbcl]
      # pois_x_base, pois_y_base = trn_x[sbcl], trn_y[sbcl]
      p_trn_x, p_trn_y = trn_x[trn_sbcl], trn_y[trn_sbcl] 

      non_p_t_x, non_p_t_y = tst_x[tst_non_sbcl], tst_y[tst_non_sbcl]
      # non_pois_x_base, non_pois_y_base = trn_x[non_sbcl], trn_y[non_sbcl]
      non_p_trn_x, non_p_trn_y = trn_x[trn_non_sbcl], trn_y[trn_non_sbcl] 

      sc_lr_pred, sc_nn_pred = lm_preds[trn_sbcl].mean(), nn_preds[trn_sbcl].mean()
      trn_ct = trn_sbcl[0].shape[0]
      tst_ct = p_t_x.shape[0]

      print(sc_lr_pred, sc_nn_pred)
      all_errs = []

      feat_lists = [prot_col for v, prot_col in zip(subcl, prot_cols) if v > 0.5]
      print(feat_lists)
      featue_name = ''
      for i in range(len(feat_lists)):
        col1 = feat_lists[i]
        col1 = col1.replace(" ", "") # remove white spaces
        if i < len(feat_lists)-1:
          featue_name = featue_name + col1 + '_'
        else:
          featue_name = featue_name + col1
      
      # also store the feature names
      feat_names.append(featue_name)
      # calculate the values of all clusters
      if compute_metric_feature:
        print("----------------- now printing the selected features for subpopulation --------------------")        
        tst_compact = compactness(p_t_x)
        trn_compact = compactness(p_trn_x)

        # test all the metrics at once
        metrics = ['min','max','avg']
        tst_sep = []
        trn_sep = []
        base_sep = []
        for metric in metrics:
          tst_sep.append(separability(p_t_x,[non_p_t_x],metric)[0])
          trn_sep.append(separability(p_trn_x,[non_p_trn_x],metric)[0])
          # base_sep.append(separability(pois_x_base,[non_pois_x_base],metric)[0])

        # print("base_compact:{}, base_sep(min):{}, base_sep(max):{},base_sep(avg):{}, train_compact:{}, train_sep(min):{},train_sep(max): {}, train_sep(avg):{},tst_compact:{}, test_sep(min):{},test_sep(max):{},test_sep(avg):{}".format(
        #   base_compact,base_sep[0],base_sep[1],base_sep[2],trn_compact,trn_sep[0],trn_sep[1],trn_sep[2],
        #   tst_compact,tst_sep[0],tst_sep[1],tst_sep[2]))
        print("train_compact:{}, train_sep(min):{},train_sep(max): {}, train_sep(avg):{},tst_compact:{}, test_sep(min):{},test_sep(max):{},test_sep(avg):{}".format(
          trn_compact,trn_sep[0],trn_sep[1],trn_sep[2],
          tst_compact,tst_sep[0],tst_sep[1],tst_sep[2]))

        # record the compactnesss, sep info of clusters
        # hand_designed_metric.append([count, base_compact,base_sep[0],base_sep[1],base_sep[2],
        # len(trn_sbcl[0]),trn_compact,trn_sep[0],trn_sep[1],trn_sep[2],len(tst_sbcl[0]),tst_compact,
        # tst_sep[0],tst_sep[1],tst_sep[2]])

        hand_designed_metric.append([count, trn_compact,trn_sep[0],trn_sep[1],trn_sep[2],len(tst_sbcl[0]),tst_compact,
        tst_sep[0],tst_sep[1],tst_sep[2]])

      for pois_ct in [int(trn_ct*pois_rate) for pois_rate in pois_rates]:
        # pois_inds = np.random.choice(pois_x_base.shape[0], pois_ct, replace=True)
        # pois_x, pois_y = pois_x_base[pois_inds], 1-pois_y_base[pois_inds]
        pois_inds = np.random.choice(p_trn_x.shape[0], pois_ct, replace=True)
        pois_x, pois_y = p_trn_x[pois_inds], 1-p_trn_y[pois_inds]

        full_x, full_y = np.concatenate((trn_x, pois_x), axis=0), np.concatenate((trn_y, pois_y), axis=0)
        print([prot_col for v, prot_col in zip(subcl, prot_cols) if v > 0.5], trn_ct, pois_ct, tst_ct)
        print("poison fraction:", pois_ct/trn_ct)

        if train_model_feature:
          # repeat the exp for 3 times
          # lmps = [linear_model.LogisticRegression(solver='liblinear', max_iter=500) for _ in range(3)]
          # for lmp in lmps:
          #   lmp.fit(full_x, full_y)
          # nnps = [neural_network.MLPClassifier(hidden_layer_sizes=(10,), max_iter=3000) for _ in range(3)]
          # for nnp in nnps:
          #   nnp.fit(full_x, full_y)
          lmp = linear_model.LogisticRegression(solver='liblinear', max_iter=500)
          lmp.fit(full_x, full_y)
          filename = 'models/FeatMatch_lm_{}_Size{}_PoisonFrac{:.2f}.sav'.format(featue_name,count,pois_ct/trn_ct)
          joblib.dump(lmp, filename)

          nnp = neural_network.MLPClassifier(hidden_layer_sizes=(10,), max_iter=3000) 
          nnp.fit(full_x, full_y)
          filename = 'models/FeatMatch_nn_{}_Size{}_PoisonFrac{:.2f}.sav'.format(featue_name,count,pois_ct/trn_ct)
          joblib.dump(nnp, filename)
        else:
          filename = 'models/FeatMatch_lm_{}_Size{}_PoisonFrac{:.2f}.sav'.format(featue_name,count,pois_ct/trn_ct)
          lmp = joblib.load(open(filename, 'rb'))
          filename = 'models/FeatMatch_nn_{}_Size{}_PoisonFrac{:.2f}.sav'.format(featue_name,count,pois_ct/trn_ct)
          nnp = joblib.load(open(filename, 'rb'))

        # lmp_acc = np.mean([lmp.score(tst_x, tst_y) for lmp in lmps])
        lmp_acc = lmp.score(tst_x, tst_y)
        print("lr acc {:.3f}".format(lmp_acc))

        # nnp_acc = np.mean([nnp.score(tst_x, tst_y) for nnp in nnps])
        nnp_acc = nnp.score(tst_x, tst_y) 
        print("nn acc {:.3f}".format(nnp_acc))

        lmc_sbc = lm.score(p_t_x, p_t_y)
        print("lr cl sbc {:.3f}".format(lmc_sbc))

        # lmp_sbc = np.mean([lmp.score(p_t_x, p_t_y) for lmp in lmps])
        lmp_sbc = lmp.score(p_t_x, p_t_y) 
        print("lr sbc {:.3f}".format(lmp_sbc))

        nnc_sbc = nn.score(p_t_x, p_t_y)
        print("nn cl sbc {:.3f}".format(nnc_sbc))

        # nnp_sbc = np.mean([nnp.score(p_t_x, p_t_y) for nnp in nnps])
        nnp_sbc = nnp.score(p_t_x, p_t_y)
        print("nn sbc {:.3f}".format(nnp_sbc))
        all_errs.append((pois_ct, lmp_acc, nnp_acc, lmc_sbc, lmp_sbc, nnc_sbc, nnp_sbc))
      hand_designed.append((trn_ct, tst_ct, sc_lr_pred, sc_nn_pred, all_errs))

  if compute_metric_feature:
    # store all the subpop metric info
    filename = 'metrics/FeatMatch_metrics.txt'
    np.savetxt(filename,np.array(hand_designed_metric))
    filename = 'metrics/FeatMatch_metrics_names.txt'
    np.savetxt(filename,np.array(feat_names),fmt = '%s')
  else:
    filename = 'metrics/FeatMatch_metrics.txt'
    hand_designed_metric = np.loadtxt(filename)
    filename = 'metrics/FeatMatch_metrics_names.txt'
    # feat_names = np.loadtxt(filename,dtype= = 'str')  
    feat_names = np.genfromtxt(filename,dtype='str')

##################### Starts the process of clustering based subpopulation ################
from sklearn import cluster
km = cluster.KMeans(n_clusters=100,random_state = 0)
km.fit(trn_x)
tst_km = km.predict(tst_x)
# trn_km = km.predict(trn_x)
kmeans_designed = []
kmeans_designed_metric = []

print(np.unique(km.labels_, return_counts=True))
# subclass and number of points in each cluster (i.e., subpop)
cl_inds, cl_cts = np.unique(km.labels_, return_counts=True)

for cl_ind, cl_ct in zip(cl_inds, cl_cts):
  if cl_ct > subpop_size_min and cl_ct < subpop_size_max:
    print("cluster indx:{}, cluster size:{}, test_cluster_size:{}".format(cl_ind, cl_ct, np.where(tst_km==cl_ind)[0].shape[0]))
    ## to produce info by each of the clusters ##
    tst_restpop = []
    trn_restpop = []
    base_restpop = []
    if cluster_wise:
      for cl_idx, cl_cnt in zip(cl_inds, cl_cts):
        if cl_idx == cl_ind: continue
        tst_sbcl = np.where(tst_km==cl_idx)
        trn_sbcl = np.where(km.labels_==cl_idx)
        # trn_sbcl = np.where(trn_km==cl_idx)

        # get the corresponding instances of the dataset
        p_t_x, p_t_y = tst_x[tst_sbcl], tst_y[tst_sbcl]
        # pois_x_base, pois_y_base = ho_x[sbcl], ho_y[sbcl]
        p_trn_x, p_trn_y = trn_x[trn_sbcl], trn_y[trn_sbcl]
        # append to the 
        tst_restpop.append(p_t_x)
        trn_restpop.append(p_trn_x)
        # base_restpop.append(pois_x_base)

    # indices of points belong to cluster
    tst_sbcl = np.where(tst_km==cl_ind)
    trn_sbcl = np.where(km.labels_==cl_ind)
    # trn_sbcl = np.where(trn_km==cl_ind)
    tst_non_sbcl = np.where(tst_km!=cl_ind)
    trn_non_sbcl = np.where(km.labels_!=cl_ind)
    # trn_non_sbcl = np.where(trn_km!=cl_ind)

    # get the corresponding instances of the dataset
    p_t_x, p_t_y = tst_x[tst_sbcl], tst_y[tst_sbcl]
    non_p_t_x, non_p_t_y = tst_x[tst_non_sbcl], tst_y[tst_non_sbcl]
    # pois_x_base, pois_y_base = ho_x[sbcl], ho_y[sbcl]
    # non_pois_x_base, pois_y_base = ho_x[non_sbcl], ho_y[non_sbcl]
    p_trn_x, p_trn_y = trn_x[trn_sbcl], trn_y[trn_sbcl]
    non_p_trn_x, non_p_trn_y = trn_x[trn_non_sbcl], trn_y[trn_non_sbcl]    

    sc_lr_pred, sc_nn_pred = lm_preds[trn_sbcl].mean(), nn_preds[trn_sbcl].mean()
    if compute_metric_cluster:
      print("----------------- now printing the selected features for subpopulation --------------------")
      print("**************************************************************************************************")
      if cluster_wise:
        # first compute the distance of cluster-wise distance
        tst_sep = separability(p_t_x,tst_restpop)
        trn_sep = separability(p_trn_x,trn_restpop)
        # base_sep = separability(pois_x_base,base_restpop) 
        # print("ClUSTER-WISE: base_sep:",base_sep)
        print("ClUSTER-WISE: train_sep:",trn_sep)
        print("ClUSTER-WISE: test_sep:",tst_sep)

      tst_compact = compactness(p_t_x)
      trn_compact = compactness(p_trn_x)
      # base_compact = compactness(pois_x_base)
      tst_sep = separability(p_t_x,[non_p_t_x])
      trn_sep = separability(p_trn_x,[non_p_trn_x])
      # base_sep = separability(pois_x_base,[non_pois_x_base])

      # test all the metrics at once
      metrics = ['min','max','avg']
      tst_sep = []
      trn_sep = []
      # base_sep = []
      for metric in metrics:
        tst_sep.append(separability(p_t_x,[non_p_t_x],metric)[0])
        trn_sep.append(separability(p_trn_x,[non_p_trn_x],metric)[0])
        # base_sep.append(separability(pois_x_base,[non_pois_x_base],metric)[0])
      # print("base_compact:{}, base_sep(min):{}, base_sep(max):{},base_sep(avg):{}, train_compact:{}, train_sep(min):{},train_sep(max): {}, train_sep(avg):{},tst_compact:{}, test_sep(min):{},test_sep(max):{},test_sep(avg):{}".format(
      #   base_compact,base_sep[0],base_sep[1],base_sep[2],trn_compact,trn_sep[0],trn_sep[1],trn_sep[2],
      #   tst_compact,tst_sep[0],tst_sep[1],tst_sep[2]))
      print("train_compact:{}, train_sep(min):{},train_sep(max): {}, train_sep(avg):{},tst_compact:{}, test_sep(min):{},test_sep(max):{},test_sep(avg):{}".format(
        trn_compact,trn_sep[0],trn_sep[1],trn_sep[2],tst_compact,tst_sep[0],tst_sep[1],tst_sep[2]))
      # print("base_compact:{}, base_sep:{}, train_compact:{}, train_sep:{}, tst_compact:{}, test_sep:{}".format(
      #   base_compact,base_sep[0],trn_compact,trn_sep[0],tst_compact,tst_sep[0]))
      print("**************************************************************************************************")

      # kmeans_designed_metric.append([cl_ind,cl_ct,base_compact,base_sep[0],base_sep[1],base_sep[2],
      # len(trn_sbcl[0]),trn_compact,trn_sep[0],trn_sep[1],trn_sep[2],
      # len(tst_sbcl[0]),tst_compact,tst_sep[0],tst_sep[1],tst_sep[2]])
      kmeans_designed_metric.append([cl_ind,cl_ct,trn_compact,trn_sep[0],trn_sep[1],trn_sep[2],
      len(tst_sbcl[0]),tst_compact,tst_sep[0],tst_sep[1],tst_sep[2]])
    trn_ct = trn_sbcl[0].shape[0]
    tst_ct = p_t_x.shape[0]
    print("Avg prediction confidence: ",sc_lr_pred, sc_nn_pred)
    all_errs = []
    
    for pois_ct in [int(trn_ct*pois_rate) for pois_rate in pois_rates]:
      # pois_inds = np.random.choice(pois_x_base.shape[0], pois_ct, replace=True)
      # pois_x, pois_y = pois_x_base[pois_inds], 1-pois_y_base[pois_inds]
      pois_inds = np.random.choice(p_trn_x.shape[0], pois_ct, replace=True)
      pois_x, pois_y = p_trn_x[pois_inds], 1-p_trn_y[pois_inds]
      full_x, full_y = np.concatenate((trn_x, pois_x), axis=0), np.concatenate((trn_y, pois_y), axis=0)
      print("poison fraction:", pois_ct/trn_ct, trn_ct, pois_ct, tst_ct)
      
      if train_model_cluster:
        # lmps = [linear_model.LogisticRegression(solver='liblinear', max_iter=500) for _ in range(3)]
        # for lmp in lmps:
        #   lmp.fit(full_x, full_y)
        lmp = linear_model.LogisticRegression(solver='liblinear', max_iter=500)
        lmp.fit(full_x, full_y)
        filename = 'models/Cluster_lm_ClusterInd{}_Size{}_PoisonFrac{:.2f}.sav'.format(cl_ind,cl_ct,pois_ct/trn_ct)
        joblib.dump(lmp, filename)
  
        # nnps = [neural_network.MLPClassifier(hidden_layer_sizes=(10,), max_iter=3000) for _ in range(3)]
        # for nnp in nnps:
        #   nnp.fit(full_x, full_y)
        nnp = neural_network.MLPClassifier(hidden_layer_sizes=(10,), max_iter=3000)
        nnp.fit(full_x, full_y)
        filename = 'models/Cluster_nn_ClusterInd{}_Size{}_PoisonFrac{:.2f}.sav'.format(cl_ind,cl_ct,pois_ct/trn_ct)
        joblib.dump(nnp, filename)
      else:
        filename = 'models/Cluster_lm_ClusterInd{}_Size{}_PoisonFrac{:.2f}.sav'.format(cl_ind,cl_ct,pois_ct/trn_ct)
        lmp = joblib.load(open(filename, 'rb'))
        filename = 'models/Cluster_nn_ClusterInd{}_Size{}_PoisonFrac{:.2f}.sav'.format(cl_ind,cl_ct,pois_ct/trn_ct)
        nnp = joblib.load(open(filename, 'rb'))

      # lmp_acc = np.mean([lmp.score(tst_x, tst_y) for lmp in lmps])
      lmp_acc = lmp.score(tst_x, tst_y)
      print("lr acc {:.3f}".format(lmp_acc))

      # nnp_acc = np.mean([nnp.score(tst_x, tst_y) for nnp in nnps])
      nnp_acc = nnp.score(tst_x, tst_y) 
      print("nn acc {:.3f}".format(nnp_acc))

      lmc_sbc = lm.score(p_t_x, p_t_y)
      print("lr cl sbc {:.3f}".format(lmc_sbc))

      # lmp_sbc = np.mean([lmp.score(p_t_x, p_t_y) for lmp in lmps])
      lmp_sbc = lmp.score(p_t_x, p_t_y) 
      print("lr sbc {:.3f}".format(lmp_sbc))

      nnc_sbc = nn.score(p_t_x, p_t_y)
      print("nn cl sbc {:.3f}".format(nnc_sbc))

      # nnp_sbc = np.mean([nnp.score(p_t_x, p_t_y) for nnp in nnps])
      nnp_sbc = nnp.score(p_t_x, p_t_y)
      print("nn sbc {:.3f}".format(nnp_sbc))
      all_errs.append((pois_ct, lmp_acc, nnp_acc, lmc_sbc, lmp_sbc, nnc_sbc, nnp_sbc))
    kmeans_designed.append((trn_ct, tst_ct, sc_lr_pred, sc_nn_pred, cl_ind, all_errs))

sys.exit(0)
# hand_designed is the FeatureMatch selection.
print(len(kmeans_designed), len(hand_designed))
if compute_metric_cluster:
  # store all the subpop metric info
  filename = 'metrics/Cluster_metrics.txt'
  np.savetxt(filename,np.array(kmeans_designed_metric))
else:
  filename = 'metrics/FeatMatch_metrics.txt'
  kmeans_designed_metric = np.loadtxt(filename)
 
nn_acc = nn.score(tst_x, tst_y)

hd_computed = []
kd_computed = []
# :hd_computed: shape -> (num of csubpop,poison_ratio)
# :kd_computed: shape -> (num of csubpop,poison_ratio)

##### record the target damage and collat damage related info ######

for hd in hand_designed:
  # hd is the record of all different subpopulations 
  trn_ct, tst_ct, sc_lr_pred, sc_nn_pred, all_errs = hd
  this_computed = []
  for err in all_errs:
    pois_ct, lmp_acc, nnp_acc, lmc_sbc, lmp_sbc, nnc_sbc, nnp_sbc = err
    target = (nnc_sbc - nnp_sbc)
    nontarget_clean = (nn_acc*tst_x.shape[0] - tst_ct*nnc_sbc)/(tst_x.shape[0]-tst_ct)
    nontarget_pois = (nnp_acc*tst_x.shape[0] - tst_ct*nnp_sbc)/(tst_x.shape[0]-tst_ct)
    collat = nontarget_clean - nontarget_pois
    this_computed.append(target)
    this_computed.append(collat)
    this_computed.append(nnc_sbc)
    this_computed.append(nnp_sbc)
    this_computed.append(pois_ct)

  hd_computed.append(this_computed)

for kd in kmeans_designed:
  trn_ct, tst_ct, sc_lr_pred, sc_nn_pred, cl_ind, all_errs = kd
  this_computed = []
  for err in all_errs:
    pois_ct, lmp_acc, nnp_acc, lmc_sbc, lmp_sbc, nnc_sbc, nnp_sbc = err
    target = (nnc_sbc - nnp_sbc)
    nontarget_clean = (nn_acc*tst_x.shape[0] - tst_ct*nnc_sbc)/(tst_x.shape[0]-tst_ct)
    nontarget_pois = (nnp_acc*tst_x.shape[0] - tst_ct*nnp_sbc)/(tst_x.shape[0]-tst_ct)
    collat = nontarget_clean - nontarget_pois
    this_computed.append(target)
    this_computed.append(collat)
    this_computed.append(nnc_sbc)
    this_computed.append(nnp_sbc)
    this_computed.append(pois_ct)
  kd_computed.append(this_computed)

hd_np = np.array(hd_computed)
kd_np = np.array(kd_computed)
hd_metric = np.array(hand_designed_metric)
kd_metric = np.array(kmeans_designed_metric)

print(kd_np.shape)  #, kd_np.shape)
collats = []
targets = []
orig_acc = []
end_accs = []
pois_size = []


#################### figure out the top-k results for illustration (no mean required) ##################################
fixed_ratio = 5/3
for top_k in [-1,-2,-3,-4,-5]:
  print(top_k)
  # this collat keeps track of collat value of hd and kd methods in top-k results for different poison ratios # 
  this_collat, this_target, this_acc, this_end_accs, this_size = [], [], [], [], []
  i = 4 # i.e., the ratio of 5/3

  # this is selecting the value of given value (e.g., target) for different poison ratios stored in different ratios
  topk_idx = np.argpartition(hd_np[:, 5*i], top_k)[top_k]
  hd_target = hd_np[:, 5*i][topk_idx]
  hd_collat = hd_np[:, 5*i + 1][topk_idx]
  hd_acc = hd_np[:, 5*i + 2][topk_idx]
  hd_end_acc = hd_np[:, 5*i + 3][topk_idx]
  hd_size = hd_np[:, 5*i + 4][topk_idx]
  hd_metrics = hd_metric[topk_idx]
  print("******* FeatMatch Full Acc Info of Highest Target Damage: Top{}, Ratio{}, Target{}, Collat{}, Clean Acc{}, Poison Count{} **********".format(
  -top_k,fixed_ratio,hd_target, hd_collat, hd_acc, hd_size))
  print("FeatMatch Metric Info of Highest Target Damage:",hd_metrics)
  # now get cluster with highest collat damage
  topk_idx = np.argpartition(hd_np[:, 5*i+1], top_k)[top_k]
  hd_target = hd_np[:, 5*i][topk_idx]
  hd_collat = hd_np[:, 5*i + 1][topk_idx]
  hd_acc = hd_np[:, 5*i + 2][topk_idx]
  hd_end_acc = hd_np[:, 5*i + 3][topk_idx]
  hd_size = hd_np[:, 5*i + 4][topk_idx]
  hd_metrics = hd_metric[topk_idx]
  print("******* FeatMatch Full Acc Info of Highest Collat Damage: Top{}, Ratio{}, Target{}, Collat{}, Clean Acc{}, Poison Count{} **********".format(
  -top_k,fixed_ratio,hd_target, hd_collat, hd_acc, hd_size))
  print("FeatMatch Metric Info of Highest Target Damage:",hd_metrics)

  ## now work on the clustering based approach ##
  topk_idx = np.argpartition(kd_np[:, 5*i], top_k)[top_k]
  kd_target = kd_np[:, 5*i][topk_idx]
  kd_collat = kd_np[:, 5*i + 1][topk_idx]
  kd_acc = kd_np[:, 5*i + 2][topk_idx]
  kd_end_acc = kd_np[:, 5*i + 3][topk_idx]
  kd_size = kd_np[:, 5*i + 4][topk_idx]
  kd_metrics = kd_metric[topk_idx]
  print("******* Cluster Full Acc Info of Highest Target Damage: Top{}, Ratio{}, Target{}, Collat{}, Clean Acc{}, Poison Count{} **********".format(
  -top_k,fixed_ratio,kd_target, kd_collat, kd_acc, kd_size))
  print("Cluster Metric Info of Highest Target Damage:",kd_metrics)
  # now get cluster with highest collat damage
  topk_idx = np.argpartition(kd_np[:, 5*i+1], top_k)[top_k]
  kd_target = kd_np[:, 5*i][topk_idx]
  kd_collat = kd_np[:, 5*i + 1][topk_idx]
  kd_acc = kd_np[:, 5*i + 2][topk_idx]
  kd_end_acc = kd_np[:, 5*i + 3][topk_idx]
  kd_size = kd_np[:, 5*i + 4][topk_idx]
  kd_metrics = kd_metric[topk_idx]
  print("******* Cluster Full Acc Info of Highest Collat Damage: Top{}, Ratio{}, Target{}, Collat{}, Clean Acc{}, Poison Count{} **********".format(
  -top_k,fixed_ratio,kd_target, kd_collat, kd_acc, kd_size))
  print("Cluster Metric Info of Highest Target Damage:",kd_metrics)

sys.exit(0)

################### figure out the top-k results for illustration ##################################
for top_k in [-1, -3, -6]:
  print(top_k)
  # this collat keeps track of collat value of hd and kd methods in top-k results for different poison ratios # 
  this_collat, this_target, this_acc, this_end_accs, this_size = [], [], [], [], []
  for i in range(6):
    # this is selecting the value of given value (e.g., target) for different poison ratios stored in different ratios
    top5 = np.argpartition(hd_np[:, 5*i], top_k)[top_k:]
    hd_target = hd_np[:, 5*i][top5].mean()
    hd_collat = hd_np[:, 5*i + 1][top5].mean()
    hd_acc = hd_np[:, 5*i + 2][top5].mean()
    hd_end_acc = hd_np[:, 5*i + 3][top5].mean()
    hd_size = hd_np[:, 5*i + 4][top5].mean()

    print(hd_target, hd_collat, hd_acc, hd_size)
    top5 = np.argpartition(kd_np[:, 5*i], top_k)[top_k:]
    kd_target = kd_np[:, 5*i][top5].mean()
    kd_collat = kd_np[:, 5*i + 1][top5].mean()
    kd_acc = kd_np[:, 5*i + 2][top5].mean()
    kd_end_acc = kd_np[:, 5*i + 3][top5].mean()
    kd_size = kd_np[:, 5*i + 4][top5].mean()
    print(kd_target, kd_collat, kd_acc, kd_size)
    
    #this_collat.append(kd_collat)
    #this_target.append(kd_target)
    #this_acc.append(kd_acc)
    #this_size.append(kd_size)
    
    this_collat.append((hd_collat, kd_collat))
    this_target.append((hd_target, kd_target))
    this_acc.append((hd_acc, kd_acc))
    this_end_accs.append((hd_end_acc, kd_end_acc))
    this_size.append((hd_size, kd_size))
    
  collats.append(this_collat)
  targets.append(this_target)
  orig_acc.append(this_acc)
  end_accs.append(this_end_accs)
  pois_size.append(this_size)

# pretty_printing
for coll, targ, orig, end_a, p_size in zip(collats, targets, orig_acc, end_accs, pois_size):
  print("a")
  hd_pretty = []
  kd_pretty = []
  for c, t, o, e, ct in zip(coll, targ, orig, end_a, p_size):# these are mean results given top-k and given poison ratios...
    hd_pretty.append((round(1000*t[0])/1000, round(1000*c[0])/1000, round(1000*o[0])/1000, round(1000*e[0])/1000, round(1000*ct[0])/1000))
    kd_pretty.append((round(1000*t[1])/1000, round(1000*c[1])/1000, round(1000*o[1])/1000, round(1000*e[1])/1000, round(1000*ct[1])/1000))
  print(hd_pretty)
  print(kd_pretty)

kd_np = np.array(kd_computed)
hd_np = np.array(hd_computed)

# we want to store cluster ind, accuracy
top6_clusters = []
top_k = 6
target_inds = [i*5 for i in range(6)]
print(kd_np[:, target_inds].max(axis=1).shape)
best_clusters = np.argpartition(kd_np[:, target_inds].max(axis=1), -top_k)[-top_k:]
best_cluster_data = kd_np[best_clusters, :] # top-k clusters with highest target value

for best_cluster in best_clusters:
  acc = kd_np[best_cluster, 2]
  cl_size = kd_np[best_cluster, 5*2 + 4]
  new_accs = [kd_np[best_cluster, 5*i + 3] for i in range(6)]

  clt_id, clt_cnt, base_compact,base_sep,trn_compact,trn_sep,tst_compact,tst_sep = kd_metric[best_cluster]
  print("cl_ind: {}, orig_acc: {}, size: {}, att_acc: {}".format(best_cluster, acc, cl_size, new_accs))
  print("-------now printing metric info----------")
  print("cl_ind: {}, base_compact: {}, base_sep: {}, trn_compact: {}, trn_sep: {}, tst_compact: {}, tst_sep: {}".format(
    best_cluster,base_compact,base_sep,trn_compact,trn_sep,tst_compact,tst_sep))

top6_clusters = []
top_k = 6
target_inds = [i*5 for i in range(6)]
print(hd_np[:, target_inds].max(axis=1).shape)
best_clusters = np.argpartition(hd_np[:, target_inds].max(axis=1), -top_k)[-top_k:]
best_cluster_data = hd_np[best_clusters, :]

for best_cluster in best_clusters:
  acc = hd_np[best_cluster, 2]
  cl_size = hd_np[best_cluster, 5*2 + 4]
  new_accs = [hd_np[best_cluster, 5*i + 3] for i in range(6)]
  count, base_compact,base_sep,trn_compact,trn_sep,tst_compact,tst_sep = hd_metric[best_cluster]
  print("cl_ind: {}, orig_acc: {}, size: {}, att_acc: {}".format(best_cluster, acc, cl_size, new_accs))
  print("-------now printing metric info----------")
  print("cl_ind: {}, base_compact: {}, base_sep: {}, trn_compact: {}, trn_sep: {}, tst_compact: {}, tst_sep: {}".format(
    best_cluster,base_compact,base_sep,trn_compact,trn_sep,tst_compact,tst_sep))

import matplotlib.pyplot as plt
tops = [1, 3, 6]
plt.figure(figsize=(5,5))
markers = ['o', 'v']
for top_k, (target, collat) in enumerate(zip(targets, collats)):
  if top_k==0:
    continue
  print(top_k, target, collat)
  plt.plot([0, 0.33, 0.66, 1, 1.33, 1.66, 2], [0] + [t for t in target], label="top {} cmatch".format(tops[top_k]), marker=markers[top_k-1], markersize=8)
  #plt.plot([0, 0.25, 0.5, 1, 1.5], [0] + [t[0] for t in target], label="top {} fmatch".format(tops[top_k]), marker=markers[top_k-1], markersize=8)
  #plt.plot([0, 0.25, 0.5, 1, 1.5], [0] + [t[1] for t in target], label="top {} cmatch".format(tops[top_k]), linestyle='--', marker=markers[top_k-1], markersize=8)
plt.legend(fontsize=14)
plt.xlabel("Within-cluster poison rate", fontsize=14)
plt.xticks([0, 0.33, 0.66, 1, 1.33, 1.66, 2], fontsize=14)
plt.yticks(fontsize=14)
plt.ylabel("Target", fontsize=14)
plt.ylim(0, 1)
plt.xlim(0, 2)
plt.tight_layout()
plt.show()

k = 25

top6 = np.argsort(hd_np[:, 6])[-k:]
hd_target = hd_np[:, 6][top6]
hd_collat = hd_np[:, 7][top6]

subcl_names = []

for i, (subcl, count) in enumerate(zip(subclasses, counts)):
  if count > 10 and count < 100:
      tst_sbcl = np.where(np.linalg.norm(tst_prot - subcl, axis=1)==0)
      mean_y = tst_y[tst_sbcl].mean()

      subcl_names.append([prot_col for v, prot_col in zip(subcl, prot_cols) if v > 0.5] + [mean_y])

for i, cn in enumerate(top6):
  trn_ct, tst_ct, sc_lr_pred, sc_nn_pred, _ = hand_designed[cn]
  print(trn_ct, tst_ct, sc_lr_pred, sc_nn_pred, hd_target[i], hd_collat[i], subcl_names[cn])
  
print()
top6 = np.argpartition(kd_np[:, 6], -k)[-k:]
kd_target = kd_np[:, 6][top6]
kd_collat = kd_np[:, 7][top6]
for i, cn in enumerate(top6):
  trn_ct, tst_ct, sc_lr_pred, sc_nn_pred, cl_ind, _ = kmeans_designed[cn]
  print(trn_ct, tst_ct, sc_lr_pred, sc_nn_pred, cl_ind, kd_target[i], kd_collat[i])

k = kd_np.shape[0]

top6 = np.argsort(kd_np[:, 6])[-k:]
kd_target = kd_np[:, 6][top6]
kd_collat = kd_np[:, 7][top6]
subclasses, counts = np.unique(trn_prot, axis=0, return_counts=True)
tst_prot = np.concatenate((tst_x[:, 12:27], tst_x[:, 52:57]), axis=1)
prot_cols = all_cols[12:27] + all_cols[52:57]


print([prot_col for v, prot_col in zip(subcl, prot_cols) if v > 0.5], trn_ct, pois_ct, tst_ct)
print(all_cols[:4])
for i, cn in enumerate(top6):
  print(cn)
  trn_ct, tst_ct, sc_lr_pred, sc_nn_pred, cl_ind, _ = kmeans_designed[cn]
  print(trn_ct, tst_ct, sc_lr_pred, sc_nn_pred, kd_target[i], kd_collat[i])
  print(tst_prot.shape, tst_km.shape, np.where(tst_km==cl_ind)[0].shape)
  for prot_col_tst in tst_x[np.where(tst_km==cl_ind)]:
    print([prot_col for v, prot_col in zip(prot_col_tst[4:], all_cols[4:]) if v > 0.5], [prot_col_tst[:4]])
    #print([prot_col for v, prot_col in zip(prot_col_tst, prot_cols) if v > 0.5])

  
