# -*- coding: utf-8 -*-
"""subpopulation_adult_Suya.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fF85onHUcLrHM8q3Ymi1RWWbiKHYzobQ
"""

# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data
# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test
import numpy as np
import pandas as pd
from sklearn import neural_network, linear_model, cluster
from sklearn.externals import joblib
import sys
from utils import *
import argparse
import csv
import scipy.io as sio
import pickle

np.random.seed(0)

parser = argparse.ArgumentParser()
parser.add_argument("--subpop_criteria", default="size", choices=["size", "variance"], help="the type of constraint when filtering the subpopulation")
parser.add_argument("--poison_type", default="untargeted", choices=["targeted", "untargeted"], help="the type of constraint when filtering the subpopulation")
parser.add_argument("--num_cluster", type=int, default=30, help = "number of clusters")
args = vars(parser.parse_args())

load_cluster_labels = True
save_data = False # if true, save the train and test data (preprocessed)
fixed_point = False
large_size = True
# featurematch cluster related params
FeatureMatch = False
compute_metric_feature = False
train_model_feature = True
save_model_feature = False
FeatMatch_size_min = 100
FeatMatch_size_max = 130
FeatMatch_var_min = 2.0
FeatMatch_var_max = 2.2
# clustering related params
Cluster = True
compute_metric_cluster = False
train_model_cluster = True
save_model_cluster = False
cluster_wise = False
Cluster_size_min = 90
Cluster_size_max = 110
Cluster_var_min = 1.5
Cluster_var_max = 1.7
# general params
train_model = False
save_model = True

if args["poison_type"] == "targeted":
  attack_type = 'targeted'
else:
  attack_type = 'untargeted'
##############################################################################
########################## Start the Real Analysis Part ######################
##############################################################################
# load data
trn_x, trn_y, tst_x, tst_y, full = load_data()
data_ops = {}
data_ops['lower'] = 0.0
data_ops['upper'] = 1.0
data_ops['sparse'] = 0
data_ops['integer'] = 0
adult_data = {}
adult_data['X_train'] = trn_x
adult_data['y_train'] = (2*trn_y-1).T
adult_data['X_test'] = tst_x
adult_data['y_test'] = (2*tst_y-1).T
adult_data['dataOpts'] = data_ops

print(np.amax(trn_x),np.amin(tst_x))
if save_data:
  sio.savemat('../data-poisoning-journal-release/data_poisoning_data/data/adult_data.mat',adult_data)
  pfile = open('../data-poisoning-journal-release/data_poisoning_data/data/adult_data','wb')
  pickle.dump(adult_data,pfile,protocol=2)

print(trn_x.shape, trn_y.shape, tst_x.shape, tst_y.shape)
print(np.amin(tst_y),np.amax(tst_y),np.amin(trn_y),np.amax(trn_y))

## train model on clean data
if train_model:
  lm = linear_model.LogisticRegression()
  lm.fit(trn_x, trn_y)
  # save the clean logistic regression model
  filename = 'models/clean_lr.sav'
  joblib.dump(lm, filename)
  # predict the values
  lm_preds = np.multiply(lm.predict_proba(trn_x), np.eye(2)[trn_y.astype(np.int)]).sum(axis=1)
  print(lm.score(tst_x, tst_y))

  nn = neural_network.MLPClassifier(hidden_layer_sizes=(10,))
  nn.fit(trn_x, trn_y)
  if save_model:
    # store the clean neural network model
    filename = 'models/clean_nn.sav'
    joblib.dump(nn, filename)
  # predict the values
  nn_preds = np.multiply(nn.predict_proba(trn_x), np.eye(2)[trn_y.astype(np.int)]).sum(axis=1)
  print(nn.score(tst_x, tst_y))

else:
  filename = 'models/clean_lr.sav'
  lm = joblib.load(open(filename, 'rb'))
  print(lm.score(tst_x, tst_y))
  # predict the values
  lm_preds = np.multiply(lm.predict_proba(trn_x), np.eye(2)[trn_y.astype(np.int)]).sum(axis=1)
  print(lm.score(tst_x, tst_y))

  filename = 'models/clean_nn.sav'
  nn = joblib.load(open(filename, 'rb'))
  print(nn.score(tst_x, tst_y))
  # predict the values
  nn_preds = np.multiply(nn.predict_proba(trn_x), np.eye(2)[trn_y.astype(np.int)]).sum(axis=1)
  print(nn.score(tst_x, tst_y))

nn_acc = nn.score(tst_x, tst_y)
lm_acc = lm.score(tst_x, tst_y)
#### this is the part of FeatureMatch subpopulation ########
# protected = np.concatenate((trn_x[:, 12:27], trn_x[:, 52:57]), axis=1)
if args["poison_type"] == "untargeted":
  tst_prot = np.concatenate((tst_x[:, 12:27], tst_x[:, 52:57]), axis=1)
  trn_prot = np.concatenate((trn_x[:, 12:27], trn_x[:, 52:57]), axis=1)
  all_cols = list(full.columns)
  prot_cols = all_cols[12:27] + all_cols[52:57]
else:
  raise NotImplementedError

######################################################################################
###    first calculate all the metric information                     #####
######################################################################################

# because of small # of features, lots of repetitive instances
subclasses, counts = np.unique(trn_prot, axis=0, return_counts=True)
FeatMatchFrq = CountFrequency(counts)
print(FeatMatchFrq)

hand_designed_metric = []
subcl_ls = []
############### Compute the fearture matching subpopulation ############
if FeatureMatch:
  if compute_metric_feature:   
    feat_names = []
    ##################### Starts the process of clustering based feature match ################
    for i, (subcl, count) in enumerate(zip(subclasses, counts)):
      if count > 10 and count < 150:
        # one-hot valued features, so l2 dist == 0 means the specific subpopulation
        tst_sbcl = np.where(np.linalg.norm(tst_prot - subcl, axis=1)==0)
        trn_sbcl = np.where(np.linalg.norm(trn_prot-subcl, axis=1)==0)
        # incides of non-subclass
        tst_non_sbcl = np.where(np.linalg.norm(tst_prot - subcl, axis=1)!=0)
        trn_non_sbcl = np.where(np.linalg.norm(trn_prot-subcl, axis=1)!=0)
        # istances in the subpopulation and rest of subpop
        p_t_x, p_t_y = tst_x[tst_sbcl], tst_y[tst_sbcl]
        p_trn_x, p_trn_y = trn_x[trn_sbcl], trn_y[trn_sbcl] 
        non_p_t_x, non_p_t_y = tst_x[tst_non_sbcl], tst_y[tst_non_sbcl]
        non_p_trn_x, non_p_trn_y = trn_x[trn_non_sbcl], trn_y[trn_non_sbcl] 

        trn_ct = trn_sbcl[0].shape[0]
        tst_ct = p_t_x.shape[0]

        all_errs = []
        feat_lists = [prot_col for v, prot_col in zip(subcl, prot_cols) if v > 0.5]
        print(feat_lists)
        featue_name = ''
        for j in range(len(feat_lists)):
          col1 = feat_lists[j]
          col1 = col1.replace(" ", "") # remove white spaces
          if j < len(feat_lists)-1:
            featue_name = featue_name + col1 + '_'
          else:
            featue_name = featue_name + col1
        
        # also store the feature names
        feat_names.append(featue_name)
        # calculate the values of all clusters
        tst_compact = compactness(p_t_x)
        trn_compact = compactness(p_trn_x)

        # test all the metrics at once
        metrics = ['min','max','avg']
        tst_sep = []
        trn_sep = []
        base_sep = []
        for metric in metrics:
          tst_sep.append(separability(p_t_x,[non_p_t_x],metric)[0])
          trn_sep.append(separability(p_trn_x,[non_p_trn_x],metric)[0])
        hand_designed_metric.append([count, trn_compact,trn_sep[0],trn_sep[1],trn_sep[2],len(tst_sbcl[0]),tst_compact,
        tst_sep[0],tst_sep[1],tst_sep[2]])
        subcl_ls.append(subcl)

    # store all the subpop metric info
    filename = 'metrics/FeatMatch_metrics_{}.txt'.format(attack_type)
    np.savetxt(filename,np.array(hand_designed_metric))
    filename = 'metrics/FeatMatch_metrics_names_{}.txt'.format(attack_type)
    np.savetxt(filename,np.array(feat_names),fmt = '%s')
    filename = 'metrics/FeatMatch_metrics_subcls{}.txt'.format(attack_type)
    np.savetxt(filename,np.array(subcl_ls),fmt='%d')
  else:
    filename = 'metrics/FeatMatch_metrics_{}.txt'.format(attack_type)
    hand_designed_metric = np.loadtxt(filename)
    filename = 'metrics/FeatMatch_metrics_names_{}.txt'.format(attack_type)
    # feat_names = np.loadtxt(filename,dtype= = 'str')  
    feat_names = np.genfromtxt(filename,dtype='str')
    filename = 'metrics/FeatMatch_metrics_subcls{}.txt'.format(attack_type)
    subcl_ls = np.loadtxt(filename)

num_clusters = args["num_cluster"]
if not load_cluster_labels:
  from sklearn import cluster
  km = cluster.KMeans(n_clusters=num_clusters,random_state = 0)
  km.fit(trn_x)
  tst_km = km.predict(tst_x)
  trn_km = km.labels_
  print("save clustering results")
  np.savetxt('metrics/trn_cluster_labels.txt',km.labels_)
  np.savetxt('metrics/tst_cluster_labels.txt',tst_km)
else:
  trn_km = np.loadtxt('metrics/trn_cluster_labels.txt')
  tst_km = np.loadtxt('metrics/tst_cluster_labels.txt')

# trn_km = km.predict(trn_x)
kmeans_designed = []
kmeans_designed_metric = []

print(np.unique(trn_km, return_counts=True))
# subclass and number of points in each cluster (i.e., subpop)
cl_inds, cl_cts = np.unique(trn_km, return_counts=True)
ClusterFrq = CountFrequency(cl_cts)
print(ClusterFrq)

if num_clusters < 100:
  sel_size_min = 0
  sel_size_max = len(trn_x)
else:
  sel_size_min = 10
  sel_size_max = 150

## print the info of clustering based subpopulation ## 
if Cluster:
  if compute_metric_cluster:
    for i, (cl_ind, cl_ct) in enumerate(zip(cl_inds, cl_cts)):
      if cl_ct > sel_size_min and cl_ct < sel_size_max:
        print("cluster indx:{}, cluster size:{}, test_cluster_size:{}".format(cl_ind, cl_ct, np.where(tst_km==cl_ind)[0].shape[0]))
        ## to produce info by each of the clusters ##
        tst_restpop = []
        trn_restpop = []
        base_restpop = []
        if cluster_wise:
          for cl_idx, cl_cnt in zip(cl_inds, cl_cts):
            if cl_idx == cl_ind: continue
            tst_sbcl = np.where(tst_km==cl_idx)
            trn_sbcl = np.where(trn_km==cl_idx)

            # get the corresponding instances of the dataset
            p_t_x, p_t_y = tst_x[tst_sbcl], tst_y[tst_sbcl]
            p_trn_x, p_trn_y = trn_x[trn_sbcl], trn_y[trn_sbcl]
            # append rest subpop one by one
            tst_restpop.append(p_t_x)
            trn_restpop.append(p_trn_x)
        
        # indices of points belong to cluster
        tst_sbcl = np.where(tst_km==cl_ind)
        trn_sbcl = np.where(trn_km==cl_ind)
        tst_non_sbcl = np.where(tst_km!=cl_ind)
        trn_non_sbcl = np.where(trn_km!=cl_ind)

        # get the corresponding instances of the dataset
        p_t_x, p_t_y = tst_x[tst_sbcl], tst_y[tst_sbcl]
        non_p_t_x, non_p_t_y = tst_x[tst_non_sbcl], tst_y[tst_non_sbcl]
        p_trn_x, p_trn_y = trn_x[trn_sbcl], trn_y[trn_sbcl]
        non_p_trn_x, non_p_trn_y = trn_x[trn_non_sbcl], trn_y[trn_non_sbcl]    

        sc_lr_pred, sc_nn_pred = lm_preds[trn_sbcl].mean(), nn_preds[trn_sbcl].mean()
        if cluster_wise:
          # first compute the distance of cluster-wise distance
          tst_sep = separability(p_t_x,tst_restpop)
          trn_sep = separability(p_trn_x,trn_restpop)
          # print("ClUSTER-WISE: train_sep:",trn_sep)
          # print("ClUSTER-WISE: test_sep:",tst_sep)
        tst_compact = compactness(p_t_x)
        trn_compact = compactness(p_trn_x)
        tst_sep = separability(p_t_x,[non_p_t_x])
        trn_sep = separability(p_trn_x,[non_p_trn_x])
        # test all the metrics at once
        metrics = ['min','max','avg']
        tst_sep = []
        trn_sep = []
        # base_sep = []
        for metric in metrics:
          tst_sep.append(separability(p_t_x,[non_p_t_x],metric)[0])
          trn_sep.append(separability(p_trn_x,[non_p_trn_x],metric)[0])
        kmeans_designed_metric.append([cl_ind,cl_ct,trn_compact,trn_sep[0],trn_sep[1],trn_sep[2],
        len(tst_sbcl[0]),tst_compact,tst_sep[0],tst_sep[1],tst_sep[2]])

    # store all the subpop metric info
    filename = 'metrics/Cluster_metrics_{}_size{}.txt'.format(attack_type,num_clusters)
    np.savetxt(filename,np.array(kmeans_designed_metric))
  else:
    filename = 'metrics/Cluster_metrics_{}_size{}.txt'.format(attack_type,num_clusters)
    kmeans_designed_metric = np.loadtxt(filename)

######################################################################################
###    selectively calculate the model training information                     ######
######################################################################################
pois_rates = [1/3, 1/2, 2/3, 1, 4/3, 5/3] # temporarily changed it as this
pois_ct_ls = [10,40,70,100]
# pois_rates = [1/3, 2/3, 1, 4/3, 5/3, 2]
#pois_rates = [0.5, 1, 1.5, 2, 2.5, 3]
target_file = open('results/large_subpop_target_feat_RatioMin-{:.1f}_RatioMax-{:.1f}.csv'.format(min(pois_rates),max(pois_rates)), 'w')
collat_file = open('results/large_subpop_collat_feat_RatioMin-{:.1f}_RatioMax-{:.1f}.csv'.format(min(pois_rates),max(pois_rates)), 'w')
target_info_writer = csv.writer(target_file, delimiter=' ')
collat_info_writer = csv.writer(collat_file, delimiter=' ')

hand_designed = []
if FeatureMatch:
  feat_names = []
  ##################### Starts the process of clustering based feature match ################
  # for i, (subcl, count) in enumerate(zip(subclasses, counts)):
  for i in range(len(subcl_ls)):
    count, trn_compact,trn_sep_0,trn_sep_1,trn_sep_2, tst_size,tst_compact,tst_sep_0,tst_sep_1,tst_sep_2 = hand_designed_metric[i]
    subcl = subcl_ls[i]
    if fixed_point:
      sel_metric = count
      filter_min = min(pois_ct_ls)
      filter_max = max(pois_ct_ls)
    else:
      if args["subpop_criteria"] == "size":
        sel_metric = count
        filter_min = FeatMatch_size_min
        filter_max = FeatMatch_size_max
      else:
        sel_metric = trn_compact
        filter_min = FeatMatch_var_min
        filter_max = FeatMatch_var_max
    if sel_metric > filter_min and sel_metric < filter_max:
      # one-hot valued features, so l2 dist == 0 means the specific subpopulation
      tst_sbcl = np.where(np.linalg.norm(tst_prot - subcl, axis=1)==0)
      trn_sbcl = np.where(np.linalg.norm(trn_prot-subcl, axis=1)==0)

      # istances in the subpopulation and rest of subpop
      p_t_x, p_t_y = tst_x[tst_sbcl], tst_y[tst_sbcl]
      p_trn_x, p_trn_y = trn_x[trn_sbcl], trn_y[trn_sbcl] 

      sc_lr_pred, sc_nn_pred = lm_preds[trn_sbcl].mean(), nn_preds[trn_sbcl].mean()
      trn_ct = trn_sbcl[0].shape[0]
      tst_ct = p_t_x.shape[0]

      all_errs = []
      feat_lists = [prot_col for v, prot_col in zip(subcl, prot_cols) if v > 0.5]
      print(feat_lists)
      print("----------------- now printing the selected features for subpopulation --------------------") 
      print("**************************************************************************************************")
      # trn_size, trn_compact,trn_sep_0,trn_sep_1,trn_sep_2, tst_size,tst_compact,tst_sep_0,tst_sep_1,tst_sep_2 = hand_designed_metric[i]    
      assert count == trn_ct
      assert tst_ct == tst_size
      print("idx:{}, train_size:{}, train_compact:{}, train_sep(min):{},train_sep(max): {}, train_sep(avg):{},tst_size:{}, tst_compact:{}, test_sep(min):{},test_sep(max):{},test_sep(avg):{}".format(
          i, count, trn_compact,trn_sep_0,trn_sep_1,trn_sep_2,tst_size,tst_compact,tst_sep_0,tst_sep_1,tst_sep_2))
      print("**************************************************************************************************")
      
      cluster_write_info_target = [count,trn_compact,tst_size,tst_compact]
      cluster_write_info_collat = [count,trn_sep_0,trn_sep_1,trn_sep_2,tst_size,tst_sep_0,tst_sep_1,tst_sep_2]
      if not fixed_point:
        pois_ct_ls = [int(trn_ct*pois_rate) for pois_rate in pois_rates]
      for pois_ct in pois_ct_ls:
        # behave differently on fixed point or fixed ratio poisoning
        if pois_ct < p_trn_x.shape[0]:
          pois_inds = np.random.choice(p_trn_x.shape[0], pois_ct, replace=False)
        else:
          pois_inds = np.random.choice(p_trn_x.shape[0], pois_ct, replace=True)

        pois_x, pois_y = p_trn_x[pois_inds], 1-p_trn_y[pois_inds]        

        full_x, full_y = np.concatenate((trn_x, pois_x), axis=0), np.concatenate((trn_y, pois_y), axis=0)
        print([prot_col for v, prot_col in zip(subcl, prot_cols) if v > 0.5], trn_ct, pois_ct, tst_ct)
        print("poison fraction and other info:", pois_ct/trn_ct, trn_ct, pois_ct, tst_ct)

        if train_model_feature:
          lmp = linear_model.LogisticRegression(solver='liblinear', max_iter=500)
          lmp.fit(full_x, full_y)
          if save_model_feature:
            filename = 'models/FeatMatch_lm_{}_Size{}_PoisonFrac{:.2f}.sav'.format(featue_name,count,pois_ct/trn_ct)
            joblib.dump(lmp, filename)

          nnp = neural_network.MLPClassifier(hidden_layer_sizes=(10,), max_iter=3000) 
          nnp.fit(full_x, full_y)
          if save_model_feature:
            filename = 'models/FeatMatch_nn_{}_Size{}_PoisonFrac{:.2f}.sav'.format(featue_name,count,pois_ct/trn_ct)
            joblib.dump(nnp, filename)
        else:
          filename = 'models/FeatMatch_lm_{}_Size{}_PoisonFrac{:.2f}.sav'.format(featue_name,count,pois_ct/trn_ct)
          lmp = joblib.load(open(filename, 'rb'))
          filename = 'models/FeatMatch_nn_{}_Size{}_PoisonFrac{:.2f}.sav'.format(featue_name,count,pois_ct/trn_ct)
          nnp = joblib.load(open(filename, 'rb'))

        lmp_acc = lmp.score(tst_x, tst_y)
        print("lr acc {:.3f}".format(lmp_acc))

        nnp_acc = nnp.score(tst_x, tst_y) 
        print("nn acc {:.3f}".format(nnp_acc))

        lmc_sbc = lm.score(p_t_x, p_t_y)
        print("lr cl sbc {:.3f}".format(lmc_sbc))

        lmp_sbc = lmp.score(p_t_x, p_t_y) 
        print("lr sbc {:.3f}".format(lmp_sbc))

        nnc_sbc = nn.score(p_t_x, p_t_y)
        print("nn cl sbc {:.3f}".format(nnc_sbc))

        nnp_sbc = nnp.score(p_t_x, p_t_y)
        print("nn sbc {:.3f}".format(nnp_sbc))

        print("---print important statistical information---")
        nontarget_clean = (lm_acc*tst_x.shape[0] - tst_ct*lmc_sbc)/(tst_x.shape[0]-tst_ct)
        nontarget_pois = (lmp_acc*tst_x.shape[0] - tst_ct*lmp_sbc)/(tst_x.shape[0]-tst_ct)
        lm_collat = nontarget_clean - nontarget_pois
        print("Size of Subpop:{}, # of poison:{}, lmp target damage:{:.3f}".format(count,pois_ct,lmc_sbc - lmp_sbc))
        print("Size of Subpop:{}, # of poison:{}, lmp collat damage:{:.3f}".format(count,pois_ct,lm_collat)) 

        nontarget_clean = (nn_acc*tst_x.shape[0] - tst_ct*nnc_sbc)/(tst_x.shape[0]-tst_ct)
        nontarget_pois = (nnp_acc*tst_x.shape[0] - tst_ct*nnp_sbc)/(tst_x.shape[0]-tst_ct)
        nn_collat = nontarget_clean - nontarget_pois
        print("Size of Subpop:{}, # of poison:{}, nnp target damage:{:.3f}".format(count,pois_ct,nnc_sbc - nnp_sbc))
        print("Size of Subpop:{}, # of poison:{}, nnp collat damage:{:.3f}".format(count,pois_ct,nn_collat))  

        # add the classification results to cvsn writer file
        cluster_write_info_target = cluster_write_info_target + [lmc_sbc - lmp_sbc,nnc_sbc - nnp_sbc]
        cluster_write_info_collat = cluster_write_info_collat + [lm_collat,nn_collat]

        all_errs.append((pois_ct, lmp_acc, nnp_acc, lmc_sbc, lmp_sbc, nnc_sbc, nnp_sbc))
      hand_designed.append((trn_ct, tst_ct, sc_lr_pred, sc_nn_pred, all_errs))
      
      target_info_writer.writerow(cluster_write_info_target)
      collat_info_writer.writerow(cluster_write_info_collat)
  for file in [target_file,collat_file]:
    file.flush()
    file.close()


# print the trained classifier information for clustering based subpop
if Cluster:
  # for i, (cl_ind, cl_ct) in enumerate(zip(cl_inds, cl_cts)):
  for i in range(len(kmeans_designed_metric)):
    cl_ind,cl_ct,trn_compact,trn_sep_0,trn_sep_1,trn_sep_2,tst_size,tst_compact,tst_sep_0,tst_sep_1,tst_sep_2 = kmeans_designed_metric[i]
    if large_size:
      sel_metric = cl_ct
      filter_min = 0
      filter_max = 1e10
    elif fixed_point:
      sel_metric = cl_ct
      filter_min = min(pois_ct_ls)
      filter_max = max(pois_ct_ls)
    else:
      if args["subpop_criteria"] == "size":
        sel_metric = cl_ct
        filter_min = FeatMatch_size_min
        filter_max = FeatMatch_size_max
      else:
        sel_metric = trn_compact
        filter_min = FeatMatch_var_min
        filter_max = FeatMatch_var_max

    print("minmax:",filter_min,filter_max)
    if sel_metric > filter_min and sel_metric < filter_max:
      print("cluster indx:{}, cluster size:{}, test_cluster_size:{}".format(cl_ind, cl_ct, np.where(tst_km==cl_ind)[0].shape[0]))

      # indices of points belong to cluster
      tst_sbcl = np.where(tst_km==cl_ind)
      trn_sbcl = np.where(trn_km==cl_ind)
      tst_nsbcl = np.where(tst_km!=cl_ind)
      trn_nsbcl = np.where(trn_km!=cl_ind)
      # get the corresponding instances of the dataset
      p_t_x, p_t_y = tst_x[tst_sbcl], tst_y[tst_sbcl]
      p_trn_x, p_trn_y = trn_x[trn_sbcl], trn_y[trn_sbcl]
      p_t_nsub_x, p_t_nsub_y = tst_x[tst_nsbcl], tst_y[tst_nsbcl]
      p_trn_nsub_x, p_trn_nsub_y = trn_x[trn_nsbcl], trn_y[trn_nsbcl]
      # num of positive and genative samples in subpop
      tst_sub_pos_num = np.sum(p_t_y == 1)
      tst_sub_neg_num = np.sum(p_t_y == 0)
      trn_sub_pos_num = np.sum(p_trn_y == 1)
      trn_sub_neg_num = np.sum(p_trn_y == 0)
      tst_nsub_pos_num = np.sum(p_t_nsub_y == 1)
      tst_nsub_neg_num = np.sum(p_t_nsub_y == 0)
      trn_nsub_pos_num = np.sum(p_trn_nsub_y == 1)
      trn_nsub_neg_num = np.sum(p_trn_nsub_y == 0)

      trn_ct = trn_sbcl[0].shape[0]
      tst_ct = p_t_x.shape[0]
      print("----------------- now printing the selected features for subpopulation --------------------")
      print("**************************************************************************************************")
      assert trn_ct == cl_ct
      assert tst_ct == tst_size
      print("idx:{},trn_size:{}, train_compact:{}, train_sep(min):{},train_sep(max): {}, train_sep(avg):{},tst_size:{}, tst_compact:{}, test_sep(min):{},test_sep(max):{},test_sep(avg):{}".format(
          i, trn_ct,trn_compact,trn_sep_0,trn_sep_1,trn_sep_2,tst_size,tst_compact,tst_sep_0,tst_sep_1,tst_sep_2))
      print("**************************************************************************************************")
      cluster_write_info_target = [cl_ct,trn_compact,tst_size,tst_compact,tst_sub_pos_num,\
      tst_sub_neg_num,trn_sub_pos_num,trn_sub_neg_num]
      cluster_write_info_collat = [cl_ct,trn_sep_0,trn_sep_1,trn_sep_2,tst_size,tst_sep_0,tst_sep_1,tst_sep_2,
      tst_nsub_pos_num,tst_nsub_neg_num,trn_nsub_pos_num,trn_nsub_neg_num]

      sc_lr_pred, sc_nn_pred = lm_preds[trn_sbcl].mean(), nn_preds[trn_sbcl].mean()
      all_errs = []
      if not fixed_point:
        pois_ct_ls = [int(trn_ct*pois_rate) for pois_rate in pois_rates]
      # start to print poisoning attack with different poison ratio or count
      for pois_ct in pois_ct_ls:
        if pois_ct < p_trn_x.shape[0]:
          pois_inds = np.random.choice(p_trn_x.shape[0], pois_ct, replace=False)
        else:
          pois_inds = np.random.choice(p_trn_x.shape[0], pois_ct, replace=True)

        pois_x, pois_y = p_trn_x[pois_inds], 1-p_trn_y[pois_inds]
        full_x, full_y = np.concatenate((trn_x, pois_x), axis=0), np.concatenate((trn_y, pois_y), axis=0)
        print("poison fraction and other info:", pois_ct/trn_ct, trn_ct, pois_ct, tst_ct)
        
        if train_model_cluster:
          lmp = linear_model.LogisticRegression(solver='liblinear', max_iter=500)
          lmp.fit(full_x, full_y)
          if save_model_cluster:
            filename = 'models/Cluster_lm_ClusterInd{}_Size{}_PoisonFrac{:.2f}.sav'.format(cl_ind,cl_ct,pois_ct/trn_ct)
            joblib.dump(lmp, filename)

          nnp = neural_network.MLPClassifier(hidden_layer_sizes=(10,), max_iter=3000)
          nnp.fit(full_x, full_y)
          if save_model_cluster:
            filename = 'models/Cluster_nn_ClusterInd{}_Size{}_PoisonFrac{:.2f}.sav'.format(cl_ind,cl_ct,pois_ct/trn_ct)
            joblib.dump(nnp, filename)
        else:
          filename = 'models/Cluster_lm_ClusterInd{}_Size{}_PoisonFrac{:.2f}.sav'.format(cl_ind,cl_ct,pois_ct/trn_ct)
          lmp = joblib.load(open(filename, 'rb'))
          filename = 'models/Cluster_nn_ClusterInd{}_Size{}_PoisonFrac{:.2f}.sav'.format(cl_ind,cl_ct,pois_ct/trn_ct)
          nnp = joblib.load(open(filename, 'rb'))

        lmp_acc = lmp.score(tst_x, tst_y)
        print("lr acc {:.3f}".format(lmp_acc))

        nnp_acc = nnp.score(tst_x, tst_y) 
        print("nn acc {:.3f}".format(nnp_acc))

        lmc_sbc = lm.score(p_t_x, p_t_y)
        print("lr cl sbc {:.3f}".format(lmc_sbc))

        lmp_sbc = lmp.score(p_t_x, p_t_y) 
        print("lr sbc {:.3f}".format(lmp_sbc))

        nnc_sbc = nn.score(p_t_x, p_t_y)
        print("nn cl sbc {:.3f}".format(nnc_sbc))

        nnp_sbc = nnp.score(p_t_x, p_t_y)
        print("nn sbc {:.3f}".format(nnp_sbc))

        print("---print important statistical information---")
        nontarget_clean_lm = (lm_acc*tst_x.shape[0] - tst_ct*lmc_sbc)/(tst_x.shape[0]-tst_ct)
        nontarget_pois = (lmp_acc*tst_x.shape[0] - tst_ct*lmp_sbc)/(tst_x.shape[0]-tst_ct)
        lm_collat = nontarget_clean_lm - nontarget_pois
        print("Size of Subpop:{}, # of poison:{}, lmp target damage:{:.3f}".format(trn_ct, pois_ct,lmc_sbc - lmp_sbc))
        print("Size of Subpop:{}, # of poison:{}, lmp collat damage:{:.3f}".format(trn_ct, pois_ct,lm_collat)) 

        nontarget_clean_nn = (nn_acc*tst_x.shape[0] - tst_ct*nnc_sbc)/(tst_x.shape[0]-tst_ct)
        nontarget_pois = (nnp_acc*tst_x.shape[0] - tst_ct*nnp_sbc)/(tst_x.shape[0]-tst_ct)
        nn_collat = nontarget_clean_nn - nontarget_pois
        print("Size of Subpop:{}, # of poison:{}, nnp target damage:{:.3f}".format(trn_ct, pois_ct,nnc_sbc - nnp_sbc))
        print("Size of Subpop:{}, # of poison:{}, nnp collat damage:{:.3f}".format(trn_ct, pois_ct,nn_collat))  

        # add the classification results to cvsn writer file
        lm_tar_dmg = lmc_sbc - lmp_sbc
        nn_tar_dmg = nnc_sbc - nnp_sbc
        cluster_write_info_target = cluster_write_info_target + [lm_tar_dmg,1-lmc_sbc,lm_tar_dmg/(1-lmc_sbc),nn_tar_dmg,1-nnc_sbc,nn_tar_dmg/(1-nnc_sbc)]
        cluster_write_info_collat = cluster_write_info_collat + [lm_collat,1-nontarget_clean_lm,lm_collat/(1-nontarget_clean_lm),nn_collat,\
          1-nontarget_clean_nn,nn_collat/(1-nontarget_clean_nn)]
        # print(cluster_write_info_target)
        # print(cluster_write_info_collat)

        all_errs.append((pois_ct, lmp_acc, nnp_acc, lmc_sbc, lmp_sbc, nnc_sbc, nnp_sbc))
      kmeans_designed.append((trn_ct, tst_ct, sc_lr_pred, sc_nn_pred, cl_ind, all_errs))

      target_info_writer.writerow(cluster_write_info_target)
      collat_info_writer.writerow(cluster_write_info_collat)
  for file in [target_file,collat_file]:
    file.flush()
    file.close()


  
